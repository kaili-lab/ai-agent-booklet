# RAG 中的 Memory 管理策略

## 知识定位
本文档在 RAG 学习体系中的位置：
- **上层**：RAG 系统架构设计
- **本文**：LLM 对话历史（Memory）管理策略
- **依赖**：向量数据库（Milvus）、Embedding 模型、Token 计数
- **相关**：RAG 基础（2-rag-test）、Milvus 使用（3-milvus-test）

---

## 前言：为什么需要 Memory 管理？

### 核心矛盾

当你构建一个 RAG 对话系统时，会遇到一个根本性的矛盾：

```
LLM 的能力              vs              现实的约束
├─ 需要上下文理解对话      ←→      ├─ Token 窗口有限（如 4k, 8k, 128k）
├─ 需要记住历史信息        ←→      ├─ API 调用成本随 token 线性增长
└─ 需要长期记忆用户偏好    ←→      └─ 单次调用无法承载无限历史
```

**举个例子：**
```
第 1 轮：用户："我叫张三"
第 2 轮：用户："我喜欢编程"
第 3 轮：用户："我今年25岁"
...
第 50 轮：用户："我的名字是什么？"
```

如果你把所有 50 轮对话都发给 LLM：
- ✅ AI 能准确回答"张三"
- ❌ 消耗大量 token（可能超过窗口限制）
- ❌ 成本高昂（每次调用都重复发送历史）

如果你只发送最近 5 轮对话：
- ✅ Token 消耗小
- ❌ AI 无法回答（"张三"在第 1 轮，已被丢弃）

**这就是 Memory 管理要解决的核心问题：在有限的上下文窗口内，如何让 LLM 获得"足够的"历史信息？**

---

### 本项目的答案：三种策略的渐进演化

本项目不是讲理论，而是展示三种**实战级**的解决方案，它们代表了从简单到复杂的演化路径：

```
策略 1：截断（Truncation）
└─ 问题：历史太长怎么办？
└─ 答案：只保留最近的 N 条或 M 个 token
└─ 本质：物理删除
└─ 代价：信息丢失

      ↓ 能否保留关键信息而不是暴力删除？

策略 2：总结（Summarization）
└─ 问题：能否压缩而不是删除？
└─ 答案：用 LLM 总结旧对话，保留摘要
└─ 本质：信息压缩
└─ 代价：需要额外 LLM 调用 + 总结可能失真

      ↓ 能否按需检索而不是全量压缩？

策略 3：检索（Retrieval）
└─ 问题：能否只召回相关的历史？
└─ 答案：向量数据库存储历史，语义检索相关对话
└─ 本质：按需查找
└─ 代价：依赖向量数据库 + 检索可能不准确
```

---

### 三种策略的关系

**不是替代关系，而是组合关系：**

- **小型项目**（如客服机器人）：截断即可
- **中型项目**（如个人助理）：截断 + 总结
- **大型项目**（如企业知识库）：截断 + 检索（或三者结合）

**本项目的价值：**
通过代码演示每种策略的实现，让你理解触发条件、实现逻辑、优劣权衡。

**阅读建议：**
- 如果你在做简单 demo → 重点看"截断策略"
- 如果你在做生产项目 → 重点看"总结策略"和"检索策略"
- 如果你想理解全貌 → 按顺序阅读，理解演化逻辑

---

## 一、存储层：历史消息放哪里？

### 两种存储方式对比

| 特性 | 内存存储 (InMemory) | 文件存储 (FileSystem) |
|------|-------------------|---------------------|
| **持久化** | ❌ 重启丢失 | ✅ 重启保留 |
| **速度** | 快 | 慢（磁盘 I/O） |
| **会话恢复** | ❌ 不支持 | ✅ 支持（通过 sessionId） |
| **适用** | 开发测试、短期对话 | 生产环境、跨天会话 |

**核心洞察：** 存储层（在哪存）独立于管理策略（存多少）。内存存储也可以配合截断/总结/检索策略。

---

## 二、管理层：三种核心策略

### 2.1 截断策略（Truncation）

**本质：** 物理删除旧消息，像固定长度队列。

**两种触发方式对比：**

| 方式 | 触发条件 | 特点 | 权衡 | 适用 |
|------|---------|------|------|------|
| **按消息数量** | 超过 N 条 | 简单可预测 | 可能浪费空间或超限 | 消息长度均匀 |
| **按 Token 数量** | 超过 M tokens | 精确控制成本 | 需计算 token（性能开销） | 生产环境、长度差异大 |

**核心缺陷：** 信息永久丢失。第 1 轮的"我叫张三"被删除后，AI 无法再回答"我叫什么"。

→ **引出问题：** 能否"压缩"而不是"删除"？

---

### 2.2 总结策略（Summarization）

**本质：** 用 LLM 将旧对话压缩为摘要，保留关键信息。

**核心优势：**
```
截断策略：10 条对话 → 保留 3 条 → "张三"丢失
总结策略：10 条对话 → 总结为 1 条 → "用户叫张三，25岁，软件工程师" 保留
```

**两种触发方式对比：**

| 方式 | 触发条件 | 保留策略 | 适用 |
|------|---------|---------|------|
| **按消息数量** | 超过 6 条 | 总结前 4 条，保留最近 2 条原文 | 对话轮次多 |
| **按 Token 数量** | 超过 200 tokens | 总结旧部分，保留最近 80 tokens 原文 | 精确成本控制 |

**核心代价：**
- 每次总结需额外 LLM 调用（增加成本和延迟）
- LLM 可能遗漏细节或产生幻觉
- 仍然是"全量总结"（包含所有主题，即使当前只关心其中一个）

→ **引出问题：** 能否"按需检索"而不是"全量总结"？

---

### 2.3 检索策略（Retrieval）

**本质：** 向量数据库存储历史，语义检索相关对话。这就是 RAG 在 Memory 中的应用。

**核心优势：**
```
场景：50 轮对话涉及"机器学习、篮球、电影、编程"四个主题
用户问："我之前提到的机器学习项目进展如何？"

总结策略：返回包含所有主题的摘要（浪费 90% token）
检索策略：只返回"机器学习"相关的 2-3 轮对话（100% 相关）
```

**实现流程：**
1. **存储阶段**：每轮对话 → Embedding 向量化 → 存入 Milvus（包含 vector, content, round, timestamp）
2. **检索阶段**：用户提问 → 向量化 → Milvus 相似度搜索 → 返回 Top K → 作为上下文传给 LLM

**关键参数：**

| 参数 | 作用 | 权衡 |
|------|------|------|
| **limit（返回数量）** | 控制召回多少条历史 | limit=1 精准但信息少，limit=10 丰富但浪费 token |
| **相似度阈值** | 过滤低相关结果 | 阈值高则精准但可能漏掉信息，阈值低则噪音多 |

**核心代价：**
- 依赖向量数据库基础设施（部署 Milvus）
- 每次查询需 Embedding 调用
- 向量相似度 ≠ 100% 语义相关（可能检索到看似相关但实际无关的对话）

**本质洞察：** Memory 检索 = RAG 思想在对话历史上的应用。知识库从"外部文档"变成"对话历史"。

---

## 三、策略选择指南

### 3.1 核心对比

| 维度 | 截断 | 总结 | 检索 |
|------|------|------|------|
| **本质** | 物理删除 | 信息压缩 | 按需查找 |
| **信息保留** | ❌ 丢失 | ⚠️ 部分保留 | ✅ 完整保留 |
| **复杂度** | 简单 | 中等 | 复杂 |
| **依赖** | 无 | LLM 总结 | 向量数据库 + Embedding |
| **成本** | 无额外 | 每次总结 1 次 LLM 调用 | Milvus 部署 + Embedding |
| **适用轮次** | < 10 | 10-50 | > 50 |
| **典型场景** | 临时客服 | 个人助理 | 企业知识库 |

---

### 3.2 决策树

```
问：对话会超过 10 轮吗？
 ├─ 否 → 【截断】按消息数量即可
 └─ 是
     ↓
    问：历史信息重要吗（如客服记录、医疗咨询）？
     ├─ 否 → 【截断】按 Token 数量
     └─ 是
         ↓
        问：对话会超过 50 轮吗？
         ├─ 否 → 【总结】按 Token 触发
         └─ 是
             ↓
            问：有向量数据库基础设施吗？
             ├─ 否 → 【总结】接受全量总结的局限
             └─ 是 → 【检索】Milvus 语义检索
```

---

### 3.3 混合策略（生产环境最佳实践）

**现实场景往往需要组合：**

| 模式 | 组合 | 架构 | 适用 |
|------|------|------|------|
| **截断 + 检索** | 当前会话保留最近 10 条（截断），长期记忆存 Milvus（检索） | 双层 Memory | 长期对话、不需全量总结 |
| **总结 + 检索** | 每 20 轮总结一次存 Milvus，原始对话也存 Milvus | 优先返回总结，必要时返回原文 | 需要主题总结的长对话 |
| **三者结合** | 工作记忆（最近 5 条截断）+ 会话总结（每次会话结束）+ 长期检索（历史会话） | 三层分层架构 | 企业级对话系统 |

---

### 3.4 成本对比（假设 100 轮对话，每轮 50 tokens）

| 策略 | Token 消耗 | LLM 调用次数 | 基础设施 |
|------|-----------|-------------|---------|
| **截断** | 500（最近 10 轮） | 100 次 | 不需要 |
| **总结** | 300（总结 + 近期） | 110 次（100 轮 + 10 次总结） | 不需要 |
| **检索** | 200（检索 2 轮 + 当前） | 100 次 | 需要 Milvus |

**结论：** Token 成本：检索 < 总结 < 截断；基础设施：检索 > 总结 = 截断

---

## 四、核心认知总结

### Memory 管理的本质

**Memory 管理不是技术问题，而是"遗忘"的艺术：**

```
人类大脑                  →        LLM Memory
├─ 工作记忆（当前对话）     →        截断（强制遗忘）
├─ 短期记忆（自动过滤）     →        总结（压缩记忆）
└─ 长期记忆（按需回忆）     →        检索（RAG 思想）
```

### 没有完美的策略

**每种策略都是权衡：**

| 策略 | 牺牲了什么 | 换来了什么 |
|------|-----------|-----------|
| 截断 | 信息完整性 | 简单性 + 低成本 |
| 总结 | API 调用成本 | 信息压缩 + 关键保留 |
| 检索 | 基础设施复杂度 | 精准召回 + 无限历史 |

**关键洞察：** 选择策略就是选择"愿意放弃什么"。

---

### 从策略到架构

**本项目展示"单一策略"，生产环境需要"策略组合"：**

```
简单项目 → 截断（按 Token） + 内存存储
中型项目 → 截断 + 总结 + 文件/数据库
大型项目 → 截断（工作记忆）+ 检索（长期记忆）+ Redis（当前）+ Milvus（历史）
```

---

### 与 RAG 系统的关系

**Memory 管理是 RAG 思想在对话历史上的应用：**

```
RAG 三要素                    →        Memory 检索策略
├─ 知识库（外部文档）          →        对话历史（存储在 Milvus）
├─ 检索器（向量搜索）          →        向量相似度搜索
└─ 生成器（LLM）              →        LLM（基于检索结果生成）
```

---

## 参考资源

**LangChain 官方文档：**
- [Memory 概念介绍](https://js.langchain.com/docs/modules/memory/)
- [ChatMessageHistory API](https://js.langchain.com/docs/modules/memory/chat_messages/)

**本项目代码文件：**
- `history-test.mjs` - 内存存储演示
- `history-test2.mjs` - 文件存储演示
- `history-test3.mjs` - 会话恢复演示
- `truncation-memory.mjs` - 截断策略实现
- `summarization-memory.mjs` - 总结策略（按消息数量）
- `summarization-memory2.mjs` - 总结策略（按 Token 数量）
- `retrieval-memory.mjs` - 检索策略实现
- `insert-conversations.mjs` - 向量数据库准备脚本
